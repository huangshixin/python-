'''

softmax函数

让我们考虑一个简单分类问题，其输入的图像的高和宽均为2像素且色彩度为灰色

所以每一个像素可以使用一个标量来表示，我们将图像中的4像素分别标记为x1-x2-x3-x4

假设训练数据集中图像的真实标签为狗 这些标签分别对应离散值y1  y2 y3
'''
import torch
'''
***
为什么要使用softmax呢？
***
它的输出值和类标的个数是相同的，且可以将输出的值转化为0-1之间的数



***
怎么做到的呢？
***
你输入一个 mxn的图片，你需要把他转化成为一个一维的向量（当然也可以不转，后续使用矩阵的乘法）---向量的维度为mxn
现在你需要拿该向量去乘上不同的weight，并输出到隐藏层，----------全连接操作。
我们使用权重矩阵来保证输出为固定的个数
-------------------实际上就是隐藏层的个数、而隐藏层的个数是通过权重矩阵来控制的-------------------------



***
假设输出为t（t可以由权重矩阵的   )
***
则我们可以设置输出矩阵为nxt
输入的维度为mxn 输出为nxt，则结果是mxt，假设m=1 也就是说输入的时候把一个图片展开成一个1x（mxn)的




***
你是否常看见arg max？
***
它想表达的是最大置信度，什么是置信度，就是说输出值为某一个参数，可以使得我们能很确定，预测的结果
比如arg max(output）分别是 0.1    10，  0.1
那我们愿意相信10这个类别的分类是正确的



***
小批量样本分类的矢量计算表达式
***
为了进一步提升计算效率，我们通常对小批量的数据做矢量计算。广义上讲，给定一个小批量样本，批次为n，输入的特征数为d（dimension），输出个数为q
则W的维度=dxq，b的维度为q

怎么理解呢 我们把一个大批次的数据拆解成小批次的
假设是输入n个数据 ，，，，，，，它们的维度为d-------------想在我们需要一个权重矩阵 dxq，把他转化为q个隐藏神经元
---紧接着，我们需要偏置项的个数和隐藏神经元的个数一致 故 1xq



***
交叉熵损失函数
***
softmax这么好，为什么要使用交叉熵呢？
我们知道softmax运算将输出变成一个合法的类别预测分布。----------负数也可以变为正数，概率和为1

想要预测分类结果正确，我们其实并不需要预测概率完全等于标签概率。
比如在图像中：
如果y(i)=3y(i)=3，那么我们只需要yˆ(i)3y^​3(i)​比其他两个预测值yˆ(i)1y^​1(i)​和yˆ(i)2y^​2(i)​大就行了。即使yˆ(i)3y^​3(i)​值为0.6，不管其他两个预测值为多少，类别预测均正确。而平方损失则过于严格
H（Y预测，Y实际）=-Seagram（Y实际）log（Y预测）-----因为log中是softmax函数








模型评价（虽然你输出的值是最大的 ，但是不能保证你的模型分类是正确的，我们需要新的衡量方式--当然后续以有监督为例子）

在训练好softmax回归模型后，给定任一样本特征，就可以预测每个输出类别的概率。

通常，我们把预测概率最大的类别作为输出类别。-------如果它与真实类别（标签）一致--------，说明这次预测是正确的。

在3.6节的实验中，我们将使用准确率（accuracy）来评价模型的表现。它等于正确预测数量与总预测数量之比。




    softmax回归适用于分类问题。它使用softmax运算输出类别的概率分布。
    softmax回归是一个单层神经网络，输出个数等于分类问题中的类别个数。
    交叉熵适合衡量两个概率分布的差异。
'''